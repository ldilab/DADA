# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /datamodule: gpl-and-beir-curr.yaml
  - override /model: hybrid
  - override /tokenizer: hybrid

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# Datasets small to large
########scifact
#######tcovid`
#######scidocs
#######fiqa
#######robust
#######cqad
#######bioasq
#######climate

method_name: ???
model_type: ???
dataset: ???

teacher: splade
student: dense
tags: ["splade", "mapper"]

seed: 42

trainer:
  min_epochs: 1
  max_epochs: 1
  num_sanity_val_steps: 32
  val_check_interval: 5_000

tokenizer:
  dense:
    model_name_or_path: sentence-transformers/msmarco-distilbert-base-tas-b

model:
  model:
    bert_name_or_module:
      _target_: src.models.trainer.HybridRetrievalModel.load_from_checkpoint
      checkpoint_path: ${ckpt_path}
      strict: false
      train_loss: null
      val_loss: null

  optimizer:
    lr: 2e-5
  train_loss:
    _target_: src.losses.hybrid.Dense2SparseMapSemanticIDFNormCriterion
    norm_method: tanh_zscore_eps
    idf_path:
      ${paths.beir_dir}/${datamodule.train_dataset}/idf/idfs.json
#      ${paths.atds_dir}/${datamodule.train_dataset}/idf_v2.json
    map_method:
      max

datamodule:
  train_dataset_type: gpl
  test_dataset_type: beir
  train_batch_size: 16
  test_batch_size: 32
  train_dataset: ${dataset}
  test_datasets:
    - ${dataset}

